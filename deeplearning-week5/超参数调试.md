# 超参数调试
1. 超参数调试的处理
  - 超参数比较少的情况下，利用设置网格点的方式来调试超参数。
  - 超参数比较多的情况下，使用随机选择点的方式进行调试。因为在处理问题的时候，我们无法得知那个超参数是更重要的，所以随机的方式更为合理。

2. 选择合适的范围
  - Scale均匀随机
在超参数选择的时候，一些超参数是在一个范围内进行均匀随机取值，如隐藏层神经元结点的个数、隐藏层的层数等。但是有一些超参数的选择做均匀随机取值是不合适的，这里需要按照一定的比例在不同的小范围内进行均匀随机取值，所以在选择的时候，在不同比例范围内进行均匀随机取值，如0.001∼0.001、0.001∼0.01、0.01∼0.1、0.1∼1 范围内选择。
```python
r = -4 * np.random.rand()   # r in [-4,0]
learning_rate = 10 ** r     # 10^{r}
```

3. 超参数调试实践
在超参数调试的实际操作中，我们需要根据我们现有的计算资源来决定以什么样的方式去调试超参数，进而对模型进行改进。
  - 在计算资源有限的情况下，仅调试一个模型，每天不断优化
  - 在计算资源充足的情况下，同时并行调试多个模型，选取其中最好的模型

4. 网络中激活值的归一化
    **Batch Norm的实现**：

$$
\mu=\frac1m\sum_iz^{\left(i\right)}
$$
$$
\sigma^2=\frac1m\sum_i\left(z^{i)}-\mu\right)^2
$$
$$
\begin{array}{l}\left(i\right)=\frac{2^{\left(1\right)-\mu}}{z_{nom}}\sqrt{\sigma^2+\epsilon}\end{array}
$$
5. 在神经网络中融入Batch Norm
实现梯度下降：同样与Mini-batch 梯度下降法相同，Batch Norm同样适用于momentum、RMSprop、Adam的梯度下降法来进行参数更新。
6. Batch Norm 正则化效果
因为在使用Mini-batch梯度下降的时候，每次计算均值和偏差都是在一个Mini-batch上进行计算，而不是在整个数据样集上。这样就在均值和偏差上带来一些比较小的噪声。那么用均值和偏差计算得到的
$$
z^{\left(i\right)}
$$
也将会加入一定的噪声。
所以和Dropout相似，其在每个隐藏层的激活值上加入了一些噪声，（这里因为Dropout以一定的概率给神经元乘上0或者1）。所以和Dropout相似，Batch Norm 也有轻微的正则化效果。
7. 在测试数据上使用Batch Norm
通常的方法就是在我们训练的过程中，对于训练集的Mini-batch，使用指数加权平均，当训练结束的时候，得到指数加权平均后的均值μ和方差σ2，而这些值直接用于Batch Norm公式的计算，用以对测试样本进行预测。
8. Softmax回归
在多分类问题中，有一种 logistic regression的一般形式，叫做Softmax regression。Softmax回归可以将多分类任务的输出转换为各个类别可能的概率，从而将最大的概率值所对应的类别作为输入样本的输出类别。
