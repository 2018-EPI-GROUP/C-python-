# [ai]第三课-浅层神经网络

### 神经网络表示

神经网络三层：输入，隐藏，输出层

### 激活函数

sigmoid激活函数：除非在二元分类的输出层 不然不要使用 或者从来不用
$$
\frac{1}{1+e^{-z}}
$$
tanh激活函数：几乎在所有场合都更优秀
$$
\frac{e^z-e^{-z}}{e^z+e^{-z}}
$$
ReLU：最常用的默认激活函数
$$
g(z)=Max(0,z)
$$
带泄露的ReLU ：也是可以的
$$
g(z)=Max(0.01z,z)
$$
多尝试在保留交叉验证数据集上面跑或者在开发集上面跑，之后选择参数效果

### 随机初始化函数

不随机选择0起始，不同的隐藏层单元将相同

初始化w^2希望落在很小的数值，会有较好的斜率，加快计算速度

# [ai]第四课-深层神经网络

### 核对矩阵维数

深度神经网络反向传播的时候，一定要核对矩阵维数前后是一致的

### 为什么使用深层网络

可以把神经网络的前几层看作探测简单的函数，比如边缘，之后与后几层结合，那么总体上就能学习更多的复杂函数组成一个复杂物体
