# 优化算法

### mini—batch梯度下降法

当样本数据过大，每一次迭代将耗费大量的时间，是不现实的方案，我认为mini—batch就是将整个样本划分为小样本，再将小样本进行迭代，没进行一次所有数据的遍历，就相当于迭代了无数次，但因为分成小样本后数据可能会有较大偏差，所以会发生梯度下降不稳定，可能会上升，但总体是下降的。

选择不大不小的Mini-batch尺寸实际上学习率达到最快。

如何选取尺寸？

1. 一般小于2000个样本，直接使用batch梯度下降法，样本集较小没必要使用mini-batch。
2. 一般的mini-batch大小在64-128-256-512等（常见），如果mini-batch大小是2的次方代码运行会快一些。注意：x^ and y^要符合CPU/GPU内存，取决于你的应用方向，以及训练集大小，如果处理的mini-batch 和 CPU/GPU内存不相符，算法的表现会非常糟糕。多尝试即可。

### Momentum梯度下降法

想法：计算梯度的指数加权平均数，并利用该梯度更新权重

超参数b一般选取的值为0.9
$$
v_{dw} = bv_{dw} + (1-b)dW\\n
v_{db} = bv_{db} + (1-b)db\\n
w = w - av_{db},b = b - av_{ab}
$$

### Adam优化算法

将Momentum和RMSprop结合在一起

a：多试几次

b_1:0.9

b_2:0.999

10^(-8)最后一个参数

 
