# 第三周作业

这次学习是双层神经网络，多了一个隐藏层，这样学习方式就可以不再是线性变化了

由于对于 plt 的不熟悉，所以描绘散点图和一些图像的绘制直接看CSDN上的了

我在尝试自己对反向传播推导的时候，发现没有学习过带矩阵的求导，这里是不是要学一下矩阵论什么的？

## 以下是代码部分
 ```
import numpy as np
import matplotlib.pyplot as plt
#from testCases import *
import sklearn
import sklearn.datasets
import sklearn.linear_model
from planar_utils import plot_decision_boundary, sigmoid, load_planar_dataset, load_extra_datasets

#%matplotlib inline #如果你使用用的是Jupyter Notebook的话请取消注释。

np.random.seed(1) #设置一个固定的随机种子，以保证接下来的步骤中我们的结果是一致的。
X, Y = load_planar_dataset()

'''
plt.scatter(X[0, :], X[1, :], c=np.squeeze(Y), s=40, cmap=plt.cm.Spectral) #绘制散点图
plt.show()

clf = sklearn.linear_model.LogisticRegressionCV()
clf.fit(X.T,Y.T)
plot_decision_boundary(lambda x: clf.predict(x), X, Y) #绘制决策边界
plt.show()
plt.title("Logistic Regression") #图标题
LR_predictions  = clf.predict(X.T) #预测结果
print ("逻辑回归的准确性： %d " % float((np.dot(Y, LR_predictions) + 
		np.dot(1 - Y,1 - LR_predictions)) / float(Y.size) * 100) +
       "% " + "(正确标记的数据点所占的百分比)")
plt.show()
#根据CSDN上的方法用单层神经网络测试
'''
def tanh(z):
    return np.tanh(z)
def dtanh(z):  # tanh(z)的倒数
    return 1-np.power(np.tanh(z),2)

def sigmoid1(z):
    return 1/(1+np.exp(-z))
def jl(A,Y):
    return -(np.dot(Y,np.log(A.T)) + np.dot(1-Y,np.log(1-A.T)))
def ag(W1,W2,b1,b2,X):
    #正向传播：
    Z1 = np.dot(W1,X) + b1
    A1 = tanh(Z1)
    Z2 = np.dot(W2,A1) + b2
    A2 = sigmoid1(Z2)
    return np.round(A2)
#初始
m = 400
h = 40
W1 = np.random.randn(h,2)*0.01   
W2 = np.random.randn(1,h)*0.01
b1 = np.zeros((h,1))
b2 = 0
alpha = 1.2

for i in range(10000):
    #正向传播：
    Z1 = np.dot(W1,X) + b1
    A1 = tanh(Z1)
    Z2 = np.dot(W2,A1) + b2
    A2 = sigmoid1(Z2)

    #反向传播：
    dz2 = A2 - Y
    dw2 = (1/m)*np.dot(dz2,A1.T)
    db2 = (1/m)*np.sum(dz2,axis = 1,keepdims=True)
    dz1 = np.dot(W2.T,dz2)*dtanh(Z1)
    dw1 = (1/m)*np.dot(dz1,X.T)
    db1 = (1/m)*np.sum(dz1,axis = 1,keepdims= True)


    #更新
    W1 -= alpha*dw1
    b1 -= alpha*db1
    W2 -= alpha*dw2
    b2 -= alpha*db2

    JL = (1/m)*jl(A2,Y)
    if i % 1000 == 0:
        print(i,JL)


print ('准确率: %d' % float((np.dot(Y, A2.T) + np.dot(1 - Y, 1 - A2.T)) / float(Y.size) * 100) + '%')


#绘制边界
plot_decision_boundary(lambda x: ag(W1,W2,b1,b2,x.T), X, Y)
plt.title("Decision Boundary for hidden layer size " + str(4))
plt.show()
```