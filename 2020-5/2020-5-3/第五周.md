
# 优化算法
## mini-batch梯度下降法

使用好的优化算法可以提高效率

把所有的$m$个样本都放在一个矩阵里面

$$X=[x^{(1)},x^{(2)},x^{(3)},\dots,x^{(m)}]$$
$$Y=[y^{(1)},y^{(2)},y^{(3)},\dots,y^{(m)}]$$

以上$X$的维数是$(n_x,m)$,$Y$的维数是$(1,m)$

这样的向量化可以很快的处理样本，但是如果$m$很大的话，处理速度仍然会很慢
(这里的$m$是样本的数量，每一个$x_i$的维度是$x_n$)

我们可以先让梯度下降分处理一部分的训练集，这样可以加快训练速度

进一步，可以把一个训练集分割成多个小的训练集，这些小的子集被称为Mini-batch 

这些小的子集，依顺序命名为$X^{\{t\}}$和$Y^{\{t\}}$

- mini-batch size=m : 这个便是之前的batch梯度下降法\\
- mini-batch size=1 : 这种算法叫做随机梯度下降法

mini-batch梯度下降法 的size 大小在上述两种情况之间

如果样本的数量小于2000个，可以直接使用 batch梯度下降 算法

mini-batch size通常取$[64,512]$之间，而且取的是$2$的倍数，这样代码运行速度会快一点

$X^{\{t\}}$和$Y^{\{t\}}$的大小通常要符合CPU或者GPU的内存

## 指数加权移动平均值

$$v_t=\beta v_{t-1}+(1-\beta)\theta_t$$

可以把$v_t$认为是$t$之前$\frac1{1-\beta}$个数据的平均值

当$\beta$的值越大的时候，点$V_t$所构成的曲线就越平滑，但是曲线会整体向右移动

$$v_\theta = 0$$
$$v_\theta := \beta v+(1-\beta)\theta_t$$

## 偏差修正
由于$v_0=0$所以数据初始的时候会与正常的数据小很多

对此，有：

对于第$t$天，我们不直接使用$v_t$作为数据，我们
使用$\frac{v_t}{1-\beta^t}$来作为每天的加权平均值

但是这个对于机器学习是非必要的，因为这个仅仅影响了前期的数据
，后面的数据和修正后的数据偏差不大。

## Momentum梯度下降法
Momentum梯度下降法总是快于标准的梯度下降算法

设当前为第$t$次迭代，假设我们已经计算出来了$dw,db$正常的梯度下降法应该是：

$$w=w-\alpha dw$$

$$b=b-\alpha db$$

在Momentum梯度下降法中，我们使用一下的公式：

$$v_{dw}=\beta v_{dw}+(1-\beta)dw$$
$$v_{db}=\beta v_{db}+(1-\beta)db$$
$$w=w-\alpha v_{dw}$$
$$b=b-\alpha v_{db}$$

这是因为，每次的迭代一般都使得参数$W$更偏向与目标值，但是也会朝梯度下降垂直的方向上移动
，由于这些移动总是来回的摆动，所以可以使用均值的方法吧这些摆动消除掉，同时也可以增加梯度下降方向上的速度

这个算法中包含了两个超参数：$\alpha$与$\beta$
，其中$\beta$的常用值是$0.9$

这里的加权平均可以不用进行偏参修正

有的人使用的公式可能是不一样的：

$$v_{dw}=\beta v_{dw}+dw$$
$$v_{db}=\beta v_{db}+db$$
这里没有$(1-\beta)$，这样的话，在更新$w$和$b$的时候，$v_{dw}$与$v_{db}$就需要乘于一个
$\frac1{1-\beta}$


## RMSprop算法
全名为："root mean square prop"

可以加速梯度下降

设当前为第$t$次迭代，有：

$$S_{dw}=\beta S_{dw}+(1-\beta)dw^2$$
$$S_{db}=\beta S_{db}+(1-\beta)db^2$$
$$w=w-\alpha\frac{dw}{\sqrt{S_{dw}}}$$
$$b=b-\alpha\frac{db}{\sqrt{S_{db}}}$$

在实际操作过程中，为了避免分母为零或者非常趋近与零的情况，我们在分母上加上一个很小的数$\varepsilon$
,例如$\varepsilon = 10^{-8}$

这个算法是起到一个归一化的作用

\section{Adam算法}
这个算法是上述两个算法的结合

同样对于第$t$次迭代，有：

$$v_{dw}=\beta_1 v_{dw}+(1-\beta_1)dw$$
$$v_{db}=\beta_1 v_{db}+(1-\beta_1)db$$
$$S_{dw}=\beta_2 S_{dw}+(1-\beta_2)dw^2$$
$$S_{db}=\beta_2 S_{db}+(1-\beta_2)db^2$$
在进行Adam算法的时候，一般要进行 偏差修正 

$$v_{dw}^{corrected}=\frac{v_{dw}}{1-\beta_1^t}$$
$$v_{db}^{corrected}=\frac{v_{db}}{1-\beta_1^t}$$
$$S_{dw}^{corrected}=\frac{S_{dw}}{1-\beta_2^t}$$
$$S_{db}^{corrected}=\frac{S_{db}}{1-\beta_2^t}$$
$$w=w-\alpha\frac{v_{dw}^{corrected}}{\sqrt{S_{dw}^{corrected}}+\varepsilon}$$
$$b=b-\alpha\frac{v_{db}^{corrected}}{\sqrt{S_{db}^{corrected}}+\varepsilon}$$
超参数$\alpha$通常要调试，可以取一系列的值进行测试
，$\beta_1$的缺省值通常是0.9， $\beta_2$通常推荐使用0.999，$\varepsilon = 10^{-8}$

## 逐步减小学习率
随着迭代的次数增加，学习率也逐步减小
，这样做可以在初始时有较大的步骤，但是在后期可以以很小的步骤去逼近目标值

## 局部最优问题
在三维空间中，使用梯度下降法，最可能会到达局部最优点，即一个极小值点

但是在高维的空间中，局部最优点往往不是那么容易到达，最容易到达的是鞍点

所以我们可以不用考虑局部最优的问题，转而考虑鞍点

但是由于mini-batch算法本身的特性，使得网络很容易就可以走出鞍点

我们应该重点考虑的应该是一些比较平缓的函数，例如高原一样

作业：

```import numpy as np
import matplotlib.pyplot as plt
import scipy.io
import math
import sklearn
import sklearn.datasets

from opt_utils import load_params_and_grads, initialize_parameters, forward_propagation, backward_propagation
from opt_utils import compute_cost, predict, predict_dec, plot_decision_boundary, load_dataset
from testCases import *

%matplotlib inline
plt.rcParams['figure.figsize'] = (7.0, 4.0) # set default size of plots
plt.rcParams['image.interpolation'] = 'nearest'
plt.rcParams['image.cmap'] = 'gray'


# GRADED FUNCTION: update_parameters_with_gd

def update_parameters_with_gd(parameters, grads, learning_rate):
    """
    Update parameters using one step of gradient descent

    Arguments:
    parameters -- python dictionary containing your parameters to be updated:
                    parameters['W' + str(l)] = Wl
                    parameters['b' + str(l)] = bl
    grads -- python dictionary containing your gradients to update each parameters:
                    grads['dW' + str(l)] = dWl
                    grads['db' + str(l)] = dbl
    learning_rate -- the learning rate, scalar.

    Returns:
    parameters -- python dictionary containing your updated parameters 
    """

    L = len(parameters) // 2 # number of layers in the neural networks

    # Update rule for each parameter
    for l in range(L):
        ### START CODE HERE ### (approx. 2 lines)
        parameters["W" + str(l+1)] = parameters["W"+str(l+1)]-learning_rate*grads["dW"+str(l+1)]
        parameters["b" + str(l+1)] = parameters["b" + str(l + 1)] - learning_rate * grads["db" + str(l + 1)]
        ### END CODE HERE ###
        
    return parameters

# GRADED FUNCTION: random_mini_batches

def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):
    """
    从（X，Y）中创建一个随机的mini-batch列表
    
    参数：
        X - 输入数据，维度为(输入节点数量，样本的数量)
        Y - 对应的是X的标签，【1 | 0】（蓝|红），维度为(1,样本的数量)
        mini_batch_size - 每个mini-batch的样本数量
        
    返回：
        mini-bacthes - 一个同步列表，维度为（mini_batch_X,mini_batch_Y）
        
    """
    
    np.random.seed(seed)            # To make your "random" minibatches the same as ours
    m = X.shape[1]                  # number of training examples
    mini_batches = []
        
    # Step 1: Shuffle (X, Y)
    permutation = list(np.random.permutation(m))
    shuffled_X = X[:, permutation]
    shuffled_Y = Y[:, permutation].reshape((1,m))

    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.
    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning
    for k in range(0, num_complete_minibatches):
        ### START CODE HERE ### (approx. 2 lines)
        mini_batch_X = shuffled_X[:,k*mini_batch_size:(k+1)*mini_batch_size]
        mini_batch_Y = shuffled_Y[:,k*mini_batch_size:(k+1)*mini_batch_size]
        ### END CODE HERE ###
        mini_batch = (mini_batch_X, mini_batch_Y)
        mini_batches.append(mini_batch)
    
    # Handling the end case (last mini-batch < mini_batch_size)
    if m % mini_batch_size != 0:
        ### START CODE HERE ### (approx. 2 lines)
        mini_batch_X = shuffled_X[:,mini_batch_size*num_complete_minibatches:]
        mini_batch_Y = shuffled_Y[:,mini_batch_size*num_complete_minibatches:]
        ### END CODE HERE ###
        mini_batch = (mini_batch_X, mini_batch_Y)
        mini_batches.append(mini_batch)
    
    return mini_batches

# GRADED FUNCTION: initialize_velocity

def initialize_velocity(parameters):
    """
    Initializes the velocity as a python dictionary with:
                - keys: "dW1", "db1", ..., "dWL", "dbL" 
                - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters.
    Arguments:
    parameters -- python dictionary containing your parameters.
                    parameters['W' + str(l)] = Wl
                    parameters['b' + str(l)] = bl
    
    Returns:
    v -- python dictionary containing the current velocity.
                    v['dW' + str(l)] = velocity of dWl
                    v['db' + str(l)] = velocity of dbl
    """
    
    L = len(parameters) // 2 # number of layers in the neural networks
    v = {}
    
    # Initialize velocity
    for l in range(L):
        ### START CODE HERE ### (approx. 2 lines)
        v["dW" + str(l+1)] = np.zeros((np.shape(parameters["W"+str(l+1)])))
        v["db" + str(l+1)] = np.zeros((np.shape(parameters["b"+str(l+1)])))
        ### END CODE HERE ###
        
    return v
# GRADED FUNCTION: update_parameters_with_momentum

def update_parameters_with_momentum(parameters, grads, v, beta, learning_rate):
    """
    Update parameters using Momentum
    
    Arguments:
    parameters -- python dictionary containing your parameters:
                    parameters['W' + str(l)] = Wl
                    parameters['b' + str(l)] = bl
    grads -- python dictionary containing your gradients for each parameters:
                    grads['dW' + str(l)] = dWl
                    grads['db' + str(l)] = dbl
    v -- python dictionary containing the current velocity:
                    v['dW' + str(l)] = ...
                    v['db' + str(l)] = ...
    beta -- the momentum hyperparameter, scalar
    learning_rate -- the learning rate, scalar
    
    Returns:
    parameters -- python dictionary containing your updated parameters 
    v -- python dictionary containing your updated velocities
    """

    L = len(parameters) // 2 # number of layers in the neural networks
    
    # Momentum update for each parameter
    for l in range(L):
        
        ### START CODE HERE ### (approx. 4 lines)
        # compute velocities
        v["dW" + str(l+1)] = beta*v["dW"+str(l+1)]+(1-beta)*grads["dW"+str(l+1)]
        v["db" + str(l+1)] = beta*v["db"+str(l+1)]+(1-beta)*grads["db"+str(l+1)]
        # update parameters
        parameters["W" + str(l+1)] = parameters["W"+str(l+1)]-learning_rate*v["dW"+str(l+1)]
        parameters["b" + str(l+1)] = parameters["b"+str(l+1)]-learning_rate*v["db"+str(l+1)]
        ### END CODE HERE ###
        
    return parameters, v
# GRADED FUNCTION: initialize_adam

def initialize_adam(parameters) :
    """
    Initializes v and s as two python dictionaries with:
                - keys: "dW1", "db1", ..., "dWL", "dbL" 
                - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters.
    
    Arguments:
    parameters -- python dictionary containing your parameters.
                    parameters["W" + str(l)] = Wl
                    parameters["b" + str(l)] = bl
    
    Returns: 
    v -- python dictionary that will contain the exponentially weighted average of the gradient.
                    v["dW" + str(l)] = ...
                    v["db" + str(l)] = ...
    s -- python dictionary that will contain the exponentially weighted average of the squared gradient.
                    s["dW" + str(l)] = ...
                    s["db" + str(l)] = ...

    """
    
    L = len(parameters) // 2 # number of layers in the neural networks
    v = {}
    s = {}
    
    # Initialize v, s. Input: "parameters". Outputs: "v, s".
    for l in range(L):
    ### START CODE HERE ### (approx. 4 lines)
        v["dW" + str(l+1)] = np.zeros((np.shape(parameters["W"+str(l+1)])))
        v["db" + str(l+1)] = np.zeros((np.shape(parameters["b"+str(l+1)])))
        s["dW" + str(l+1)] = np.zeros((np.shape(parameters["W"+str(l+1)])))
        s["db" + str(l+1)] = np.zeros((np.shape(parameters["b"+str(l+1)])))
    ### END CODE HERE ###
    
    return v, s
# GRADED FUNCTION: update_parameters_with_adam

def update_parameters_with_adam(parameters, grads, v, s, t, learning_rate = 0.01,
                                beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8):
    """
    Update parameters using Adam
    
    Arguments:
    parameters -- python dictionary containing your parameters:
                    parameters['W' + str(l)] = Wl
                    parameters['b' + str(l)] = bl
    grads -- python dictionary containing your gradients for each parameters:
                    grads['dW' + str(l)] = dWl
                    grads['db' + str(l)] = dbl
    v -- Adam variable, moving average of the first gradient, python dictionary
    s -- Adam variable, moving average of the squared gradient, python dictionary
    learning_rate -- the learning rate, scalar.
    beta1 -- Exponential decay hyperparameter for the first moment estimates 
    beta2 -- Exponential decay hyperparameter for the second moment estimates 
    epsilon -- hyperparameter preventing division by zero in Adam updates

    Returns:
    parameters -- python dictionary containing your updated parameters 
    v -- Adam variable, moving average of the first gradient, python dictionary
    s -- Adam variable, moving average of the squared gradient, python dictionary
    """
    
    L = len(parameters) // 2                 # number of layers in the neural networks
    v_corrected = {}                         # Initializing first moment estimate, python dictionary
    s_corrected = {}                         # Initializing second moment estimate, python dictionary
    
    # Perform Adam update on all parameters
    for l in range(L):
        # Moving average of the gradients. Inputs: "v, grads, beta1". Output: "v".
        ### START CODE HERE ### (approx. 2 lines)
        v["dW" + str(l+1)] = beta1*v["dW"+str(l+1)]+(1-beta1)*grads['dW'+str(l+1)]
        v["db" + str(l+1)] = beta1*v["db"+str(l+1)]+(1-beta1)*grads['db'+str(l+1)]
        ### END CODE HERE ###

        # Compute bias-corrected first moment estimate. Inputs: "v, beta1, t". Output: "v_corrected".
        ### START CODE HERE ### (approx. 2 lines)
        v_corrected["dW" + str(l+1)] = v["dW"+str(l+1)]/(1+np.power(beta1,t))
        v_corrected["db" + str(l+1)] = v["db"+str(l+1)]/(1+np.power(beta1,t))
        ### END CODE HERE ###

        # Moving average of the squared gradients. Inputs: "s, grads, beta2". Output: "s".
        ### START CODE HERE ### (approx. 2 lines)
        s["dW" + str(l+1)] = beta2*s["dW"+str(l+1)]+(1-beta2)*np.square(grads["dW"+str(l+1)])
        s["db" + str(l+1)] = beta2 * s["db" + str(l + 1)] + (1 - beta2) * np.square(grads["db" + str(l + 1)])
        ### END CODE HERE ###

        # Compute bias-corrected second raw moment estimate. Inputs: "s, beta2, t". Output: "s_corrected".
        ### START CODE HERE ### (approx. 2 lines)
        s_corrected["dW" + str(l+1)] = s["dW" + str(l + 1)] / (1 - np.power(beta2,t))
        s_corrected["db" + str(l+1)] = s["db" + str(l + 1)] / (1 - np.power(beta2,t))
        ### END CODE HERE ###

        # Update parameters. Inputs: "parameters, learning_rate, v_corrected, s_corrected, epsilon". Output: "parameters".
        ### START CODE HERE ### (approx. 2 lines)
        parameters["W" + str(l+1)] = parameters["W" + str(l+1)]-learning_rate*((v_corrected["dW" + str(l+1)])/(np.sqrt(s_corrected["dW" + str(l+1)])+epsilon))
        parameters["b" + str(l+1)] = parameters["b" + str(l+1)]-learning_rate*((v_corrected["db" + str(l+1)])/(np.sqrt(s_corrected["db" + str(l+1)])+epsilon))
        ### END CODE HERE ###

    return parameters, v, s
def model(X, Y, layers_dims, optimizer, learning_rate = 0.0007, mini_batch_size = 64, beta = 0.9,
          beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8, num_epochs = 10000, print_cost = True):
    """
    3-layer neural network model which can be run in different optimizer modes.
    
    Arguments:
    X -- input data, of shape (2, number of examples)
    Y -- true "label" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)
    layers_dims -- python list, containing the size of each layer
    learning_rate -- the learning rate, scalar.
    mini_batch_size -- the size of a mini batch
    beta -- Momentum hyperparameter
    beta1 -- Exponential decay hyperparameter for the past gradients estimates 
    beta2 -- Exponential decay hyperparameter for the past squared gradients estimates 
    epsilon -- hyperparameter preventing division by zero in Adam updates
    num_epochs -- number of epochs
    print_cost -- True to print the cost every 1000 epochs

    Returns:
    parameters -- python dictionary containing your updated parameters 
    """

    L = len(layers_dims)             # number of layers in the neural networks
    costs = []                       # to keep track of the cost
    t = 0                            # initializing the counter required for Adam update
    seed = 10                        # For grading purposes, so that your "random" minibatches are the same as ours
    
    # Initialize parameters
    parameters = initialize_parameters(layers_dims)

    # Initialize the optimizer
    if optimizer == "gd":
        pass # no initialization required for gradient descent
    elif optimizer == "momentum":
        v = initialize_velocity(parameters)
    elif optimizer == "adam":
        v, s = initialize_adam(parameters)
    
    # Optimization loop
    for i in range(num_epochs):
        
        # Define the random minibatches. We increment the seed to reshuffle differently the dataset after each epoch
        seed = seed + 1
        minibatches = random_mini_batches(X, Y, mini_batch_size, seed)

        for minibatch in minibatches:

            # Select a minibatch
            (minibatch_X, minibatch_Y) = minibatch

            # Forward propagation
            a3, caches = forward_propagation(minibatch_X, parameters)

            # Compute cost
            cost = compute_cost(a3, minibatch_Y)

            # Backward propagation
            grads = backward_propagation(minibatch_X, minibatch_Y, caches)

            # Update parameters
            if optimizer == "gd":
                parameters = update_parameters_with_gd(parameters, grads, learning_rate)
            elif optimizer == "momentum":
                parameters, v = update_parameters_with_momentum(parameters, grads, v, beta, learning_rate)
            elif optimizer == "adam":
                t = t + 1 # Adam counter
                parameters, v, s = update_parameters_with_adam(parameters, grads, v, s,
                                                               t, learning_rate, beta1, beta2,  epsilon)
        
        # Print the cost every 1000 epoch
        if print_cost and i % 1000 == 0:
            print ("Cost after epoch %i: %f" %(i, cost))
        if print_cost and i % 100 == 0:
            costs.append(cost)
                
    # plot the cost
    plt.plot(costs)
    plt.ylabel('cost')
    plt.xlabel('epochs (per 100)')
    plt.title("Learning rate = " + str(learning_rate))
    plt.show()

    return parameters
```