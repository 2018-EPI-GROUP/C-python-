```import numpy as npimport matplotlib.pyplot as pltfrom testCases import *import sklearnimport sklearn.datasetsimport sklearn.linear_modelfrom planar_utils import plot_decision_boundary, sigmoid, load_planar_dataset, load_extra_datasetsimport operatorfrom functools import reduce np.random.seed(1) #设置一个固定的随机种子，方便检验结果X, Y = load_planar_dataset()#Y中的标签，红色为0，蓝色为1#plt.scatter(X[0, :], X[1, :], c=reduce(operator.add, Y), s=40, cmap=plt.cm.Spectral) #绘制散点图 此函数用法很玄学，还得多使用shape_X = X.shapeshape_Y = Y.shapem = Y.shape[1]  # 训练集里面的数量def layer_sizes(X , Y):    n_x = X.shape[0]  #输入层    n_h = 4           #隐藏层，硬编码为4    n_y = Y.shape[0]  #输出层    return (n_x,n_h,n_y)#随机初始化参数def initialize_parameters( n_x , n_h ,n_y):    """    参数：        n_x - 输入层节点的数量        n_h - 隐藏层节点的数量        n_y - 输出层节点的数量        返回：        parameters - 包含参数的字典：            W1 - 权重矩阵,维度为（n_h，n_x）            b1 - 偏向量，维度为（n_h，1）            W2 - 权重矩阵，维度为（n_y，n_h）            b2 - 偏向量，维度为（n_y，1）    """    np.random.seed(2) #指定一个随机种子，固定随机数    W1 = np.random.randn(n_h,n_x) * 0.01    b1 = np.zeros(shape=(n_h, 1))    W2 = np.random.randn(n_y,n_h) * 0.01    b2 = np.zeros(shape=(n_y, 1))    #使用断言确保数据格式是正确的    assert(W1.shape == ( n_h , n_x ))    assert(b1.shape == ( n_h , 1 ))    assert(W2.shape == ( n_y , n_h ))    assert(b2.shape == ( n_y , 1 ))        parameters = {"W1" : W1,	              "b1" : b1,	              "W2" : W2,	              "b2" : b2 }    return parameters#正向传播def forward_propagation( X , parameters ):    """    参数：         X - 维度为（n_x，m）的输入数据。         parameters - 初始化函数（initialize_parameters）的输出        返回：         A2 - 使用sigmoid()函数计算的第二次激活后的数值         cache - 包含“Z1”，“A1”，“Z2”和“A2”的字典类型变量     """    W1 = parameters["W1"]    b1 = parameters["b1"]    W2 = parameters["W2"]    b2 = parameters["b2"]        Z1 = np.dot(W1 , X) + b1    A1 = np.tanh(Z1)    Z2 = np.dot(W2 , A1) + b2    A2 = sigmoid(Z2)    #使用断言确保数据格式是正确的    assert(A2.shape == (1,X.shape[1]))    cache = {"Z1": Z1,             "A1": A1,             "Z2": Z2,             "A2": A2}        return (A2, cache)#计算交叉熵成本def compute_cost(A2,Y,parameters):    """    参数：         A2 - 使用sigmoid()函数计算的第二次激活后的数值         Y - "True"标签向量,维度为（1，m）         parameters - 一个包含W1，B1，W2，B2的字典类型的变量        返回：         成本    """        m = Y.shape[1]    W1 = parameters["W1"]    W2 = parameters["W2"]        #计算成本    logprobs = np.multiply(np.log(A2), Y) + np.multiply((1 - Y), np.log(1 - A2))    cost = - np.sum(logprobs) / m    cost = float(np.squeeze(cost))    #用numpy.multiply和numpy.sum 方便理解        '''    在机器学习和深度学习中，通常算法的结果是可以表示向量的数组    （即包含两对或以上的方括号形式[[]]），    如果直接利用这个数组进行画图可能显示界面为空（见后面的示例）。    我们可以利用squeeze（）函数将表示向量的数组转换为秩为1的数组，    这样利用matplotlib库函数画图时，就可以正常的显示结果了。    '''        assert(isinstance(cost,float))        return cost#反向传播def backward_propagation(parameters,cache,X,Y):    """    参数：     parameters - 包含W1,b1,W2,b2的一个字典类型的变量。     cache - 包含Z1，A1，Z2,A2的字典类型的变量。     X - 输入数据，维度为（2，m）     Y - 标签，维度为（1，m）        返回：     grads - 包含W和b的导数一个字典类型的变量。    """    m = X.shape[1]        W1 = parameters["W1"]    W2 = parameters["W2"]        A1 = cache["A1"]    A2 = cache["A2"]        dZ2= A2 - Y    dW2 = (1 / m) * np.dot(dZ2, A1.T)    db2 = (1 / m) * np.sum(dZ2, axis=1, keepdims=True)    dZ1 = np.multiply(np.dot(W2.T, dZ2), 1 - np.power(A1, 2))    dW1 = (1 / m) * np.dot(dZ1, X.T)    db1 = (1 / m) * np.sum(dZ1, axis=1, keepdims=True)    grads = {"dW1": dW1,             "db1": db1,             "dW2": dW2,             "db2": db2 }        return grads#更新参数def update_parameters(parameters,grads,learning_rate=1.2):    """     参数：     parameters - 包含W1,b1,W2,b2的字典类型的变量。     grads - 包含导数值的字典类型的变量。     learning_rate - 学习速率        返回：     parameters - 包含更新参数的字典类型的变量。    """    W1,W2 = parameters["W1"],parameters["W2"]    b1,b2 = parameters["b1"],parameters["b2"]        dW1,dW2 = grads["dW1"],grads["dW2"]    db1,db2 = grads["db1"],grads["db2"]        W1 = W1 - learning_rate * dW1    b1 = b1 - learning_rate * db1    W2 = W2 - learning_rate * dW2    b2 = b2 - learning_rate * db2        parameters = {"W1": W1,                  "b1": b1,                  "W2": W2,                  "b2": b2}        return parameters'''说句心里话，这个学习速率的选择应该是整个算法最玄学的部分，如何选择合适的学习速率这应该是之后学习的重点之一'''#进行整合，缝合怪来啦！！！def nn_model(X,Y,n_h,num_iterations,print_cost=False):    """    参数：        X - 数据集,维度为（2，示例数）        Y - 标签，维度为（1，示例数）        n_h - 隐藏层的数量        num_iterations - 梯度下降循环中的迭代次数        print_cost - 一个开关，如果为True，则每1000次迭代打印一次成本数值        返回：        parameters - 模型学习的参数，用来进行预测。     """         np.random.seed(3) #指定随机种子    n_x = layer_sizes(X, Y)[0]    n_y = layer_sizes(X, Y)[2]        parameters = initialize_parameters(n_x,n_h,n_y)    W1 = parameters["W1"]    b1 = parameters["b1"]    W2 = parameters["W2"]    b2 = parameters["b2"]        for i in range(num_iterations):        A2 , cache = forward_propagation(X,parameters)        cost = compute_cost(A2,Y,parameters)        grads = backward_propagation(parameters,cache,X,Y)        parameters = update_parameters(parameters,grads,learning_rate = 0.5)                if print_cost:            if i==0:                print("第 ",i+1," 次循环，成本为："+str(cost))            if i%1000 == 999:                print("第 ",i+1," 次循环，成本为："+str(cost))    return parameters#进行预测def predict(parameters,X):    """    参数：		parameters - 包含W1,b1,W2,b2的一个字典类型的变量。	    X - 输入数据（n_x，m）        返回		predictions - 我们模型预测的向量（红色：0 /蓝色：1）          """    A2 , cache = forward_propagation(X,parameters)    predictions = np.round(A2)    '''    np.round可以对各元素进行小数点后的四舍五入    需注意：在python3中 如果四舍五入时候到两边的距离一样，结果会是偶数部分的值    例如 np.round(0.5) = 0.0    '''    return predictions#普通的运行parameters = nn_model(X, Y, n_h = 4, num_iterations=10000, print_cost=True)#绘制边界plot_decision_boundary(lambda x: predict(parameters, x.T), X, Y[0,:])#此处有很多细节需要注意，例如传参plt.title("Decision Boundary for hidden layer size " + str(4))predictions = predict(parameters, X)print ('准确率: %d' % float((np.dot(Y, predictions.T) + np.dot(1 - Y, 1 - predictions.T)) / float(Y.size) * 100) + '%')#更改隐藏层数量，观察准确率的变化'''plt.figure(figsize=(16, 32))hidden_layer_sizes = [1, 2, 3, 4, 5, 20, 50] #隐藏层数量for i, n_h in enumerate(hidden_layer_sizes):    plt.subplot(5, 2, i + 1)    plt.title('Hidden Layer of size %d' % n_h)    parameters = nn_model(X, Y, n_h, num_iterations=5000)    plot_decision_boundary(lambda x: predict(parameters, x.T), X, Y[0,:])    predictions = predict(parameters, X)    accuracy = float((np.dot(Y, predictions.T) + np.dot(1 - Y, 1 - predictions.T)) / float(Y.size) * 100)    print ("隐藏层的节点数量： {}  ，准确率: {} %".format(n_h, accuracy))'''```**本节最难的大概就是那个绘图函数的使用了。。。令人脱发**