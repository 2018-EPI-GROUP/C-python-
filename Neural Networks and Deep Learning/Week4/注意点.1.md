##问题：
 
在deeplearning.ai的课程1『神经网络与深度学习』中的第4周的编程作业『Building your Deep Neural Network: Step by Step』中，为什么多层神经网络的权重初始化是

``
parameters["W" + str(l)] = np.random.randn(layers_dims[l], layers_dims[l - 1]) / np.sqrt(layers_dims[l-1])
``

而不是

``
parameters["W" + str(l)] = np.random.randn(layers_dims[l], layers_dims[l - 1]) * 0.01
``？
##答案：

1. 在课程2『改善深层神经网络：超参数调试、正则化以及优化』中第一周的第11讲『神经网络的权重初始化』中有详细解释：只有2层及以下神经网络的权重初始化才是*0.01，而多层神经网络都要 /np.sqrt(layers_dims[l-1])，即要除以其前一层神经元数量的平方根。
2. 原因：因为，我们希望初始时 Z[l] 要足够小，不能太大，否则会进入激活函数的饱和区使得梯度下降很慢，产生梯度消失。 但是如果第 l-1 层神经元数量n越大，那么值必须设置越小，所以某层 W[l] 值一定要同它的输入神经元数（即前一层神经元数）挂钩，因此才会有：

``
W[l]=np.random.randn(shape)*np.aqrt(1/n[l-1])
``
。

##总结：
1. Xavier初始化：如果激活函数是tanh函数，初始化时：也是要乘以np.sqrt(1/n[l-1])，这叫Xavier初始化。在一篇论文中提到出了这个。而bengio提出的初始化方法是乘以np.sqrt(2/n[l-1]+n[l])。
2. He初始化：如果激活函数是relu函数，初始化时：是要乘以np.sqrt(2/n[l-1])，因为它的方差是这个，这种初始化叫He初始化。


